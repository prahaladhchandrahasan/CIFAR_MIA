{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Idevice = \"cuda\""
      ],
      "metadata": {
        "id": "daL4DdHqaMeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "metadata": {
        "id": "TZAnEpTtX6F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset, random_split"
      ],
      "metadata": {
        "id": "GQbi24O0btM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsQe-sydpHgp",
        "outputId": "95d6bde8-6e3b-4202-b8cb-5e31a42d137b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path= '/content/drive/My Drive/ISPP_Folder/CIFAKE.zip'"
      ],
      "metadata": {
        "id": "0_D0_OY_pe-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the destination directory for unzipped files\n",
        "extract_to = '/content/unzipped_folder/'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# Open and extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Files unzipped successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ff2jyNqCs0",
        "outputId": "0bdc7814-3bbb-416c-ff74-004b3f1d55dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files unzipped successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "from torchvision import datasets\n",
        "\n",
        "def split_cifar10_into_five_splits(batch_size=64, seed=42):\n",
        "    \"\"\"\n",
        "    Splits the CIFAR-10 training dataset into 6 subsets, each containing 10,000 examples.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): The batch size for the DataLoader.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        list of DataLoader: A list containing 6 DataLoaders, each for a subset with 10,000 examples.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Define transformations and load CIFAR-10 dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    dataset = datasets.ImageFolder(root=\"/content/unzipped_folder/CIFAKE/NEW_train\", transform=transform)\n",
        "    print(dataset)\n",
        "    # Ensure dataset has exactly 60,000 examples\n",
        "    assert len(dataset) == 50000, \"Expected CIFAR-10 train dataset to contain 50,000 examples.\"\n",
        "\n",
        "    # Define lengths for each split\n",
        "    lengths = [10000] * 4 + [10000]\n",
        "\n",
        "    # Split the dataset\n",
        "    splits = random_split(dataset, lengths)\n",
        "\n",
        "    # Create DataLoaders for each split\n",
        "    loaders = [DataLoader(split, batch_size=batch_size, shuffle=True) for split in splits]\n",
        "\n",
        "    return loaders\n",
        "\n",
        "# Usage\n",
        "loaders = split_cifar10_into_five_splits()\n",
        "\n",
        "# Example: Access the first DataLoader\n",
        "for images, labels in loaders[0]:\n",
        "    print(images.shape, labels.shape)  # Each batch in the first split\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AOjo6NiYsz0",
        "outputId": "4876c399-9bb5-4d8b-88af-e2459a73b9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 50000\n",
            "    Root location: /content/unzipped_folder/CIFAKE/NEW_train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
            "           )\n",
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(loaders))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsQj3-6fpGqJ",
        "outputId": "14d863f7-fede-43e8-812b-3d63b02b7e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import torch\n",
        "# import torchvision.transforms as transforms\n",
        "# from torch.utils.data import DataLoader, Subset, random_split\n",
        "# from torchvision.datasets import CIFAR10\n",
        "\n",
        "# def split_cifar10_into_five_splits(batch_size=64, seed=42, noise_ratio=0.1):\n",
        "#     \"\"\"\n",
        "#     Splits the CIFAR-10 training dataset into 5 subsets, each containing 10,000 examples.\n",
        "#     Adds 10% label noise to the last 4 subsets.\n",
        "\n",
        "#     Args:\n",
        "#         batch_size (int): The batch size for the DataLoader.\n",
        "#         seed (int): Random seed for reproducibility.\n",
        "#         noise_ratio (float): The ratio of labels to change for noise in the last 4 subsets.\n",
        "\n",
        "#     Returns:\n",
        "#         list of DataLoader: A list containing 5 DataLoaders, each for a subset with 10,000 examples.\n",
        "#     \"\"\"\n",
        "#     # Set random seed for reproducibility\n",
        "#     torch.manual_seed(seed)\n",
        "\n",
        "#     # Define transformations and load CIFAR-10 dataset\n",
        "#     transform = transforms.Compose([\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "#     ])\n",
        "#     dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "#     # Ensure dataset has exactly 50,000 examples\n",
        "#     assert len(dataset) == 50000, \"Expected CIFAR-10 train dataset to contain 50,000 examples.\"\n",
        "\n",
        "#     # Define lengths for each split\n",
        "#     lengths = [10000] * 5\n",
        "\n",
        "#     # Split the dataset into 5 subsets\n",
        "#     splits = random_split(dataset, lengths)\n",
        "\n",
        "#     # Add label noise to the last 4 splits\n",
        "#     for i in range(1, 5):\n",
        "#         subset = splits[i]\n",
        "#         num_noisy_labels = int(noise_ratio * len(subset))\n",
        "\n",
        "#         # Randomly select indices within the subset to add noise\n",
        "#         noisy_indices = random.sample(range(len(subset)), num_noisy_labels)\n",
        "\n",
        "#         for idx in noisy_indices:\n",
        "#             # Map subset index to the original dataset index\n",
        "#             original_idx = subset.indices[idx]\n",
        "#             original_label = dataset.targets[original_idx]\n",
        "\n",
        "#             # Generate a new label that is different from the original\n",
        "#             noisy_label = random.randint(0, 9)\n",
        "#             while noisy_label == original_label:  # Ensure the new label is different\n",
        "#                 noisy_label = random.randint(0, 9)\n",
        "\n",
        "#             # Apply the noisy label to the original dataset\n",
        "#             dataset.targets[original_idx] = noisy_label\n",
        "\n",
        "#     # Create DataLoaders for each split\n",
        "#     loaders = [DataLoader(split, batch_size=batch_size, shuffle=True) for split in splits]\n",
        "\n",
        "#     return loaders\n",
        "\n",
        "# # Usage\n",
        "# loaders = split_cifar10_into_five_splits()\n",
        "\n",
        "# # Example: Access the first batch in each DataLoader\n",
        "# for i, loader in enumerate(loaders):\n",
        "#     print(f\"Loader {i+1}:\")\n",
        "#     for images, labels in loader:\n",
        "#         print(images.shape, labels.shape)  # Each batch in the split\n",
        "#         break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CTyP-IbNjqV",
        "outputId": "428a2221-25af-41e1-c2be-3c82f45a1531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Loader 1:\n",
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n",
            "Loader 2:\n",
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n",
            "Loader 3:\n",
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n",
            "Loader 4:\n",
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n",
            "Loader 5:\n",
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import torch\n",
        "# import torchvision.transforms as transforms\n",
        "# from torch.utils.data import DataLoader, Subset, random_split\n",
        "# from torchvision.datasets import CIFAR10\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#         transforms.ToTensor(),\n",
        "#          transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "#    ])\n",
        "# dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j6YD8oJbLC0",
        "outputId": "ec1a11c8-250e-441d-f109-14c90710858b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 39.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "In2QaRLPbfQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(loaders))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4RZa8VZNxEt",
        "outputId": "22f934c3-3fa7-4d2a-bfc5-7ad4b802faf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example assuming custom datasets\n",
        "for i, data_loader in enumerate(loaders):\n",
        "    dataset = data_loader.dataset  # Extract dataset from the DataLoader\n",
        "    torch.save(dataset, f\"custom_dataset_{i}.pth\")"
      ],
      "metadata": {
        "id": "qs5eGierH0or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader_configs = []\n",
        "\n",
        "for i, data_loader in enumerate(loaders):\n",
        "    config = {\n",
        "        'batch_size': data_loader.batch_size,\n",
        "        'shuffle': data_loader.dataset.indices,\n",
        "        'num_workers': data_loader.num_workers,\n",
        "        # Add other configurations if needed\n",
        "    }\n",
        "    data_loader_configs.append(config)\n",
        "\n",
        "# Save all configurations together\n",
        "torch.save(data_loader_configs, \"dataloader_configs.pth\")"
      ],
      "metadata": {
        "id": "aNFKiBMBH6ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 1e-7"
      ],
      "metadata": {
        "id": "5HcYUZu1ZOMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.tanh(self.conv1(x)))\n",
        "        x = self.pool(self.tanh(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)  # Flatten the tensor for the fully connected layer\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "T_oFrsFcZdj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_model = CNN()"
      ],
      "metadata": {
        "id": "7d3XLviUOdk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).type(torch.float32).mean()"
      ],
      "metadata": {
        "id": "RmJATpvhZd9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "pnbo-UAcZmuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "target_optimizer = optim.Adam(target_model.parameters(), lr=learning_rate, weight_decay=learning_rate_decay)"
      ],
      "metadata": {
        "id": "J2arDnAyaGbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,optimizer,loader):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print the loss after each epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "1oyJSVulZu_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(target_model,target_optimizer,train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RERBKZJsaKVO",
        "outputId": "d1832cb7-f0c5-4001-bc94-47717c865e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.9833\n",
            "Epoch [2/50], Loss: 1.8606\n",
            "Epoch [3/50], Loss: 1.8138\n",
            "Epoch [4/50], Loss: 1.7848\n",
            "Epoch [5/50], Loss: 1.7602\n",
            "Epoch [6/50], Loss: 1.7437\n",
            "Epoch [7/50], Loss: 1.7285\n",
            "Epoch [8/50], Loss: 1.7175\n",
            "Epoch [9/50], Loss: 1.7030\n",
            "Epoch [10/50], Loss: 1.6905\n",
            "Epoch [11/50], Loss: 1.6801\n",
            "Epoch [12/50], Loss: 1.6732\n",
            "Epoch [13/50], Loss: 1.6664\n",
            "Epoch [14/50], Loss: 1.6565\n",
            "Epoch [15/50], Loss: 1.6492\n",
            "Epoch [16/50], Loss: 1.6442\n",
            "Epoch [17/50], Loss: 1.6386\n",
            "Epoch [18/50], Loss: 1.6378\n",
            "Epoch [19/50], Loss: 1.6332\n",
            "Epoch [20/50], Loss: 1.6264\n",
            "Epoch [21/50], Loss: 1.6221\n",
            "Epoch [22/50], Loss: 1.6187\n",
            "Epoch [23/50], Loss: 1.6156\n",
            "Epoch [24/50], Loss: 1.6119\n",
            "Epoch [25/50], Loss: 1.6102\n",
            "Epoch [26/50], Loss: 1.6091\n",
            "Epoch [27/50], Loss: 1.6059\n",
            "Epoch [28/50], Loss: 1.6034\n",
            "Epoch [29/50], Loss: 1.6027\n",
            "Epoch [30/50], Loss: 1.6048\n",
            "Epoch [31/50], Loss: 1.5999\n",
            "Epoch [32/50], Loss: 1.5964\n",
            "Epoch [33/50], Loss: 1.5955\n",
            "Epoch [34/50], Loss: 1.5982\n",
            "Epoch [35/50], Loss: 1.5959\n",
            "Epoch [36/50], Loss: 1.5934\n",
            "Epoch [37/50], Loss: 1.5937\n",
            "Epoch [38/50], Loss: 1.5891\n",
            "Epoch [39/50], Loss: 1.5880\n",
            "Epoch [40/50], Loss: 1.5866\n",
            "Epoch [41/50], Loss: 1.5869\n",
            "Epoch [42/50], Loss: 1.5866\n",
            "Epoch [43/50], Loss: 1.5867\n",
            "Epoch [44/50], Loss: 1.5859\n",
            "Epoch [45/50], Loss: 1.5872\n",
            "Epoch [46/50], Loss: 1.5818\n",
            "Epoch [47/50], Loss: 1.5847\n",
            "Epoch [48/50], Loss: 1.5816\n",
            "Epoch [49/50], Loss: 1.5830\n",
            "Epoch [50/50], Loss: 1.5818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root=\"/content/unzipped_folder/CIFAKE/NEW_test\", transform=transform)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "lJJNNoWVf4jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(test_dataset, f\"test_dataset.pth\")"
      ],
      "metadata": {
        "id": "bO_HBiRa8w9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "def test(model,test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Append predictions and labels for metric calculation\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = 100 * correct / total\n",
        "    precision = precision_score(all_labels, all_predictions, average='macro')\n",
        "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "YectQD3Camp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(target_model,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "fQkdGGIbgG88",
        "outputId": "1260e530-0480-41a1-8fe2-d3c6f1c34210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'target_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4234e83c032f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'target_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(target_model,train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzNqpKbIgIIx",
        "outputId": "523ebab6-223c-40bd-8658-cb2f42d1223f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 89.03%\n",
            "Precision: 0.8908\n",
            "Recall: 0.8903\n",
            "F1 Score: 0.8901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(target_model.state_dict(), \"target_model.pth\")"
      ],
      "metadata": {
        "id": "Cu30utDMHGEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shadow_models = []\n",
        "shadow_optimizers = []\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for i in range(4):\n",
        "  print(\"Training Shadow model\",i+1)\n",
        "  shadow_model =   CNN()\n",
        "  shadow_optimizer = optim.Adam(shadow_model.parameters(), lr=learning_rate, weight_decay=learning_rate_decay)\n",
        "  shadow_models.append(shadow_model)\n",
        "  shadow_optimizers.append(shadow_optimizer)\n",
        "  train(shadow_model,shadow_optimizer,loaders[i+1])\n",
        "  test(shadow_model,loaders[i+1])\n",
        "  test(shadow_model,test_loader)\n",
        "  model_name = \"shadow_model\"+str(i+1)+\".pth\"\n",
        "  torch.save(shadow_model.state_dict(), model_name)"
      ],
      "metadata": {
        "id": "ShvWIrD-gfJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4cfb1c-dba1-4a46-a571-e0cd43070e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Shadow model 1\n",
            "Epoch [1/50], Loss: 1.9295\n",
            "Epoch [2/50], Loss: 1.7623\n",
            "Epoch [3/50], Loss: 1.6972\n",
            "Epoch [4/50], Loss: 1.6604\n",
            "Epoch [5/50], Loss: 1.6300\n",
            "Epoch [6/50], Loss: 1.6055\n",
            "Epoch [7/50], Loss: 1.5847\n",
            "Epoch [8/50], Loss: 1.5666\n",
            "Epoch [9/50], Loss: 1.5532\n",
            "Epoch [10/50], Loss: 1.5383\n",
            "Epoch [11/50], Loss: 1.5251\n",
            "Epoch [12/50], Loss: 1.5196\n",
            "Epoch [13/50], Loss: 1.5113\n",
            "Epoch [14/50], Loss: 1.5060\n",
            "Epoch [15/50], Loss: 1.5083\n",
            "Epoch [16/50], Loss: 1.5025\n",
            "Epoch [17/50], Loss: 1.4966\n",
            "Epoch [18/50], Loss: 1.4925\n",
            "Epoch [19/50], Loss: 1.4899\n",
            "Epoch [20/50], Loss: 1.4868\n",
            "Epoch [21/50], Loss: 1.4872\n",
            "Epoch [22/50], Loss: 1.4868\n",
            "Epoch [23/50], Loss: 1.4883\n",
            "Epoch [24/50], Loss: 1.4890\n",
            "Epoch [25/50], Loss: 1.5161\n",
            "Epoch [26/50], Loss: 1.5146\n",
            "Epoch [27/50], Loss: 1.5027\n",
            "Epoch [28/50], Loss: 1.4911\n",
            "Epoch [29/50], Loss: 1.4858\n",
            "Epoch [30/50], Loss: 1.4807\n",
            "Epoch [31/50], Loss: 1.4794\n",
            "Epoch [32/50], Loss: 1.4775\n",
            "Epoch [33/50], Loss: 1.4768\n",
            "Epoch [34/50], Loss: 1.4763\n",
            "Epoch [35/50], Loss: 1.4762\n",
            "Epoch [36/50], Loss: 1.4759\n",
            "Epoch [37/50], Loss: 1.4752\n",
            "Epoch [38/50], Loss: 1.4751\n",
            "Epoch [39/50], Loss: 1.4750\n",
            "Epoch [40/50], Loss: 1.4748\n",
            "Epoch [41/50], Loss: 1.4748\n",
            "Epoch [42/50], Loss: 1.4749\n",
            "Epoch [43/50], Loss: 1.5677\n",
            "Epoch [44/50], Loss: 1.5626\n",
            "Epoch [45/50], Loss: 1.5299\n",
            "Epoch [46/50], Loss: 1.5128\n",
            "Epoch [47/50], Loss: 1.5016\n",
            "Epoch [48/50], Loss: 1.4959\n",
            "Epoch [49/50], Loss: 1.4899\n",
            "Epoch [50/50], Loss: 1.4842\n",
            "Test Accuracy: 98.36%\n",
            "Precision: 0.9835\n",
            "Recall: 0.9837\n",
            "F1 Score: 0.9836\n",
            "Test Accuracy: 83.16%\n",
            "Precision: 0.8305\n",
            "Recall: 0.8316\n",
            "F1 Score: 0.8307\n",
            "Training Shadow model 2\n",
            "Epoch [1/50], Loss: 1.9237\n",
            "Epoch [2/50], Loss: 1.7611\n",
            "Epoch [3/50], Loss: 1.6989\n",
            "Epoch [4/50], Loss: 1.6628\n",
            "Epoch [5/50], Loss: 1.6331\n",
            "Epoch [6/50], Loss: 1.6062\n",
            "Epoch [7/50], Loss: 1.5897\n",
            "Epoch [8/50], Loss: 1.5738\n",
            "Epoch [9/50], Loss: 1.5556\n",
            "Epoch [10/50], Loss: 1.5456\n",
            "Epoch [11/50], Loss: 1.5369\n",
            "Epoch [12/50], Loss: 1.5275\n",
            "Epoch [13/50], Loss: 1.5166\n",
            "Epoch [14/50], Loss: 1.5123\n",
            "Epoch [15/50], Loss: 1.5078\n",
            "Epoch [16/50], Loss: 1.5025\n",
            "Epoch [17/50], Loss: 1.4974\n",
            "Epoch [18/50], Loss: 1.4966\n",
            "Epoch [19/50], Loss: 1.4942\n",
            "Epoch [20/50], Loss: 1.4998\n",
            "Epoch [21/50], Loss: 1.4959\n",
            "Epoch [22/50], Loss: 1.4927\n",
            "Epoch [23/50], Loss: 1.4896\n",
            "Epoch [24/50], Loss: 1.4884\n",
            "Epoch [25/50], Loss: 1.4948\n",
            "Epoch [26/50], Loss: 1.5079\n",
            "Epoch [27/50], Loss: 1.4936\n",
            "Epoch [28/50], Loss: 1.4890\n",
            "Epoch [29/50], Loss: 1.4836\n",
            "Epoch [30/50], Loss: 1.4825\n",
            "Epoch [31/50], Loss: 1.4832\n",
            "Epoch [32/50], Loss: 1.4816\n",
            "Epoch [33/50], Loss: 1.4798\n",
            "Epoch [34/50], Loss: 1.4782\n",
            "Epoch [35/50], Loss: 1.4787\n",
            "Epoch [36/50], Loss: 1.4997\n",
            "Epoch [37/50], Loss: 1.5242\n",
            "Epoch [38/50], Loss: 1.5031\n",
            "Epoch [39/50], Loss: 1.4931\n",
            "Epoch [40/50], Loss: 1.4868\n",
            "Epoch [41/50], Loss: 1.4869\n",
            "Epoch [42/50], Loss: 1.4807\n",
            "Epoch [43/50], Loss: 1.4850\n",
            "Epoch [44/50], Loss: 1.4795\n",
            "Epoch [45/50], Loss: 1.4779\n",
            "Epoch [46/50], Loss: 1.4765\n",
            "Epoch [47/50], Loss: 1.4769\n",
            "Epoch [48/50], Loss: 1.4769\n",
            "Epoch [49/50], Loss: 1.4805\n",
            "Epoch [50/50], Loss: 1.4817\n",
            "Test Accuracy: 98.21%\n",
            "Precision: 0.9821\n",
            "Recall: 0.9821\n",
            "F1 Score: 0.9821\n",
            "Test Accuracy: 81.50%\n",
            "Precision: 0.8137\n",
            "Recall: 0.8150\n",
            "F1 Score: 0.8136\n",
            "Training Shadow model 3\n",
            "Epoch [1/50], Loss: 1.9520\n",
            "Epoch [2/50], Loss: 1.7783\n",
            "Epoch [3/50], Loss: 1.7105\n",
            "Epoch [4/50], Loss: 1.6733\n",
            "Epoch [5/50], Loss: 1.6371\n",
            "Epoch [6/50], Loss: 1.6124\n",
            "Epoch [7/50], Loss: 1.5890\n",
            "Epoch [8/50], Loss: 1.5675\n",
            "Epoch [9/50], Loss: 1.5516\n",
            "Epoch [10/50], Loss: 1.5354\n",
            "Epoch [11/50], Loss: 1.5236\n",
            "Epoch [12/50], Loss: 1.5155\n",
            "Epoch [13/50], Loss: 1.5094\n",
            "Epoch [14/50], Loss: 1.5040\n",
            "Epoch [15/50], Loss: 1.4970\n",
            "Epoch [16/50], Loss: 1.4933\n",
            "Epoch [17/50], Loss: 1.4889\n",
            "Epoch [18/50], Loss: 1.4863\n",
            "Epoch [19/50], Loss: 1.4841\n",
            "Epoch [20/50], Loss: 1.4821\n",
            "Epoch [21/50], Loss: 1.4807\n",
            "Epoch [22/50], Loss: 1.4803\n",
            "Epoch [23/50], Loss: 1.4791\n",
            "Epoch [24/50], Loss: 1.4778\n",
            "Epoch [25/50], Loss: 1.4772\n",
            "Epoch [26/50], Loss: 1.5179\n",
            "Epoch [27/50], Loss: 1.5560\n",
            "Epoch [28/50], Loss: 1.5232\n",
            "Epoch [29/50], Loss: 1.5084\n",
            "Epoch [30/50], Loss: 1.4937\n",
            "Epoch [31/50], Loss: 1.4843\n",
            "Epoch [32/50], Loss: 1.4815\n",
            "Epoch [33/50], Loss: 1.4781\n",
            "Epoch [34/50], Loss: 1.4769\n",
            "Epoch [35/50], Loss: 1.4754\n",
            "Epoch [36/50], Loss: 1.4748\n",
            "Epoch [37/50], Loss: 1.4741\n",
            "Epoch [38/50], Loss: 1.4738\n",
            "Epoch [39/50], Loss: 1.4737\n",
            "Epoch [40/50], Loss: 1.4736\n",
            "Epoch [41/50], Loss: 1.4735\n",
            "Epoch [42/50], Loss: 1.4737\n",
            "Epoch [43/50], Loss: 1.4732\n",
            "Epoch [44/50], Loss: 1.4730\n",
            "Epoch [45/50], Loss: 1.4727\n",
            "Epoch [46/50], Loss: 1.4725\n",
            "Epoch [47/50], Loss: 1.4724\n",
            "Epoch [48/50], Loss: 1.4723\n",
            "Epoch [49/50], Loss: 1.4722\n",
            "Epoch [50/50], Loss: 1.4721\n",
            "Test Accuracy: 98.89%\n",
            "Precision: 0.9890\n",
            "Recall: 0.9889\n",
            "F1 Score: 0.9890\n",
            "Test Accuracy: 84.58%\n",
            "Precision: 0.8455\n",
            "Recall: 0.8458\n",
            "F1 Score: 0.8455\n",
            "Training Shadow model 4\n",
            "Epoch [1/50], Loss: 1.9291\n",
            "Epoch [2/50], Loss: 1.7774\n",
            "Epoch [3/50], Loss: 1.7153\n",
            "Epoch [4/50], Loss: 1.6687\n",
            "Epoch [5/50], Loss: 1.6308\n",
            "Epoch [6/50], Loss: 1.6085\n",
            "Epoch [7/50], Loss: 1.5878\n",
            "Epoch [8/50], Loss: 1.5644\n",
            "Epoch [9/50], Loss: 1.5485\n",
            "Epoch [10/50], Loss: 1.5363\n",
            "Epoch [11/50], Loss: 1.5250\n",
            "Epoch [12/50], Loss: 1.5219\n",
            "Epoch [13/50], Loss: 1.5112\n",
            "Epoch [14/50], Loss: 1.5052\n",
            "Epoch [15/50], Loss: 1.4992\n",
            "Epoch [16/50], Loss: 1.4943\n",
            "Epoch [17/50], Loss: 1.4906\n",
            "Epoch [18/50], Loss: 1.4889\n",
            "Epoch [19/50], Loss: 1.4908\n",
            "Epoch [20/50], Loss: 1.4896\n",
            "Epoch [21/50], Loss: 1.4865\n",
            "Epoch [22/50], Loss: 1.4855\n",
            "Epoch [23/50], Loss: 1.4824\n",
            "Epoch [24/50], Loss: 1.4795\n",
            "Epoch [25/50], Loss: 1.4788\n",
            "Epoch [26/50], Loss: 1.4792\n",
            "Epoch [27/50], Loss: 1.4771\n",
            "Epoch [28/50], Loss: 1.4765\n",
            "Epoch [29/50], Loss: 1.4760\n",
            "Epoch [30/50], Loss: 1.4757\n",
            "Epoch [31/50], Loss: 1.4766\n",
            "Epoch [32/50], Loss: 1.5443\n",
            "Epoch [33/50], Loss: 1.5795\n",
            "Epoch [34/50], Loss: 1.5396\n",
            "Epoch [35/50], Loss: 1.5204\n",
            "Epoch [36/50], Loss: 1.5021\n",
            "Epoch [37/50], Loss: 1.4898\n",
            "Epoch [38/50], Loss: 1.4834\n",
            "Epoch [39/50], Loss: 1.4796\n",
            "Epoch [40/50], Loss: 1.4761\n",
            "Epoch [41/50], Loss: 1.4744\n",
            "Epoch [42/50], Loss: 1.4736\n",
            "Epoch [43/50], Loss: 1.4734\n",
            "Epoch [44/50], Loss: 1.4736\n",
            "Epoch [45/50], Loss: 1.4728\n",
            "Epoch [46/50], Loss: 1.4726\n",
            "Epoch [47/50], Loss: 1.4724\n",
            "Epoch [48/50], Loss: 1.4723\n",
            "Epoch [49/50], Loss: 1.4725\n",
            "Epoch [50/50], Loss: 1.4721\n",
            "Test Accuracy: 98.92%\n",
            "Precision: 0.9893\n",
            "Recall: 0.9892\n",
            "F1 Score: 0.9892\n",
            "Test Accuracy: 83.76%\n",
            "Precision: 0.8369\n",
            "Recall: 0.8376\n",
            "F1 Score: 0.8371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  shadow_model =   CNN()\n",
        "  shadow_optimizer = optim.Adam(shadow_model.parameters(), lr=learning_rate, weight_decay=learning_rate_decay)\n",
        "  shadow_models.append(shadow_model)\n",
        "  shadow_optimizers.append(shadow_optimizer)\n",
        "  train(shadow_model,shadow_optimizer,loaders[0])\n",
        "  test(shadow_model,loaders[0])\n",
        "  test(shadow_model,test_loader)\n",
        "  model_name = \"shadow_model\"+str(0)+\".pth\"\n",
        "  torch.save(shadow_model.state_dict(), model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_xVd7r84fH4",
        "outputId": "7a3e4883-43c0-4798-98f1-813d7243424d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.9312\n",
            "Epoch [2/50], Loss: 1.7764\n",
            "Epoch [3/50], Loss: 1.7119\n",
            "Epoch [4/50], Loss: 1.6689\n",
            "Epoch [5/50], Loss: 1.6430\n",
            "Epoch [6/50], Loss: 1.6177\n",
            "Epoch [7/50], Loss: 1.5944\n",
            "Epoch [8/50], Loss: 1.5758\n",
            "Epoch [9/50], Loss: 1.5656\n",
            "Epoch [10/50], Loss: 1.5460\n",
            "Epoch [11/50], Loss: 1.5341\n",
            "Epoch [12/50], Loss: 1.5224\n",
            "Epoch [13/50], Loss: 1.5136\n",
            "Epoch [14/50], Loss: 1.5100\n",
            "Epoch [15/50], Loss: 1.5094\n",
            "Epoch [16/50], Loss: 1.4995\n",
            "Epoch [17/50], Loss: 1.4942\n",
            "Epoch [18/50], Loss: 1.4940\n",
            "Epoch [19/50], Loss: 1.4907\n",
            "Epoch [20/50], Loss: 1.4886\n",
            "Epoch [21/50], Loss: 1.4927\n",
            "Epoch [22/50], Loss: 1.4991\n",
            "Epoch [23/50], Loss: 1.4942\n",
            "Epoch [24/50], Loss: 1.4934\n",
            "Epoch [25/50], Loss: 1.4856\n",
            "Epoch [26/50], Loss: 1.4818\n",
            "Epoch [27/50], Loss: 1.4807\n",
            "Epoch [28/50], Loss: 1.4780\n",
            "Epoch [29/50], Loss: 1.4770\n",
            "Epoch [30/50], Loss: 1.4761\n",
            "Epoch [31/50], Loss: 1.4751\n",
            "Epoch [32/50], Loss: 1.4744\n",
            "Epoch [33/50], Loss: 1.4737\n",
            "Epoch [34/50], Loss: 1.4734\n",
            "Epoch [35/50], Loss: 1.5134\n",
            "Epoch [36/50], Loss: 1.5540\n",
            "Epoch [37/50], Loss: 1.5247\n",
            "Epoch [38/50], Loss: 1.5078\n",
            "Epoch [39/50], Loss: 1.4946\n",
            "Epoch [40/50], Loss: 1.4856\n",
            "Epoch [41/50], Loss: 1.4805\n",
            "Epoch [42/50], Loss: 1.4764\n",
            "Epoch [43/50], Loss: 1.4747\n",
            "Epoch [44/50], Loss: 1.4734\n",
            "Epoch [45/50], Loss: 1.4725\n",
            "Epoch [46/50], Loss: 1.4729\n",
            "Epoch [47/50], Loss: 1.4722\n",
            "Epoch [48/50], Loss: 1.4716\n",
            "Epoch [49/50], Loss: 1.4709\n",
            "Epoch [50/50], Loss: 1.4709\n",
            "Test Accuracy: 83.24%\n",
            "Precision: 0.8316\n",
            "Recall: 0.8324\n",
            "F1 Score: 0.8318\n",
            "Test Accuracy: 84.07%\n",
            "Precision: 0.8398\n",
            "Recall: 0.8407\n",
            "F1 Score: 0.8400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  test(shadow_model,loaders[0])\n",
        "  test(shadow_model,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtemmVAR7zqo",
        "outputId": "db17ce36-c2c9-4521-8859-8d5941900427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.08%\n",
            "Precision: 0.9908\n",
            "Recall: 0.9908\n",
            "F1 Score: 0.9908\n",
            "Test Accuracy: 84.07%\n",
            "Precision: 0.8398\n",
            "Recall: 0.8407\n",
            "F1 Score: 0.8400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_attack_data(model, data_loader, member_label):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    attack_data = []\n",
        "    attack_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            probs = model(images)\n",
        "            attack_data.append(probs.cpu().numpy())\n",
        "            attack_labels.append(member_label)\n",
        "    return attack_data, attack_labels"
      ],
      "metadata": {
        "id": "GByCI5UFPMkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_size = len(test_loader.dataset) // 5\n",
        "print(split_size)\n",
        "split_sizes = [split_size] * 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Lt3RAxQeY0Q",
        "outputId": "534ab546-26db-4123-ec6d-ba75cadd9e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size = len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "b4RHac0Sehaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Adjust the last split to account for any remainder\n",
        "if sum(split_sizes) < total_size:\n",
        "    split_sizes[-1] += total_size - sum(split_sizes)\n",
        "\n",
        "# Split the dataset into 4 parts\n",
        "subset1, subset2, subset3, subset4, subset5 = random_split(test_dataset, split_sizes)\n",
        "\n",
        "# Create DataLoaders for each subset\n",
        "batch_size = 32  # Define batch size for each DataLoader\n",
        "dataloader1 = DataLoader(subset1, batch_size=1, shuffle=True)\n",
        "dataloader2 = DataLoader(subset2, batch_size=1, shuffle=True)\n",
        "dataloader3 = DataLoader(subset3, batch_size=1, shuffle=True)\n",
        "dataloader4 = DataLoader(subset4, batch_size=1, shuffle=True)\n",
        "dataloader5 = DataLoader(subset5, batch_size=1, shuffle=True)\n",
        "\n",
        "# Now you have 4 DataLoaders with approximately equal data split\n",
        "data_loaders = [dataloader1, dataloader2, dataloader3, dataloader4, dataloader5]"
      ],
      "metadata": {
        "id": "Y6duajWTeTFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_data = []\n",
        "attack_labels = []\n",
        "resized_train_loaders =[]\n",
        "for i in range(5):\n",
        "  print(i)\n",
        "  train_loader_resized = DataLoader(dataset=loaders[i+1].dataset, batch_size=1, shuffle=True)\n",
        "  resized_train_loaders.append(train_loader_resized)\n",
        "  member_data, member_labels = generate_attack_data(shadow_models[i], train_loader_resized, member_label=1)\n",
        "  non_member_data, non_member_labels = generate_attack_data(shadow_models[i], data_loaders[i], member_label=0)\n",
        "  attack_data.extend(member_data+non_member_data)\n",
        "  attack_labels.extend(member_labels+non_member_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmoHXd71bnd0",
        "outputId": "bc8e7acf-edf7-4f62-afd9-e7a5da68b8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(attack_data))\n",
        "print(len(attack_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVmN_EIyc4cX",
        "outputId": "868b2804-fe7a-4576-a303-dd0ad98bc910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now that I have attack_data and attack_labels make a classifier to classify them. give their precision recall and F1 score too\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming attack_data and attack_labels are already defined as lists\n",
        "# Convert attack_data to a NumPy array\n",
        "attack_data = np.array(attack_data).reshape(len(attack_data), -1)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(attack_data, attack_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb-azdM8c_Us",
        "outputId": "e44d5495-ce2d-4b4b-97a1-793c1f3a1727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.808\n",
            "Recall: 1.0\n",
            "F1 Score: 0.8938053097345132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "member_loader_resized = DataLoader(dataset=train_data, batch_size=1, shuffle=True)\n",
        "member_attack_data, member_attack_labels = generate_attack_data(target_model, member_loader_resized, member_label=1)\n",
        "non_member_data, non_member_labels = generate_attack_data(shadow_model, data_loaders[2], member_label=0)\n",
        "\n",
        "attack_data = []\n",
        "attack_labels = []\n",
        "attack_data.extend(member_attack_data+non_member_data)\n",
        "attack_labels.extend(member_attack_labels+non_member_labels)\n",
        "\n",
        "attack_data = np.array(attack_data).reshape(len(attack_data), -1)\n",
        "y_test = np.array(attack_labels)\n",
        "\n",
        "y_pred = model.predict(attack_data)\n",
        "\n",
        "# Evaluate the model\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jw4r3EPfgm3",
        "outputId": "158b55c2-11f6-4bad-9e29-4a87c714c827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8\n",
            "Recall: 1.0\n",
            "F1 Score: 0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i] != y_test[i]:\n",
        "    count+=1\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajRoHVIkgwS0",
        "outputId": "8aacafd5-fccc-48ce-c5db-421c975f58e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n"
          ]
        }
      ]
    }
  ]
}